{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RSCAM Assessment 3\n",
    "\n",
    "## Neural Networks\n",
    "\n",
    "In this lab we will learn a little about the training process for neural networks.  Before beginning this lab you *must* review the materials in Lectures 5 and 6 (see slides/video on the RSCAM Learn page).  The lab builds directly on the examples (XOR and SPD Classifier) which are introduced in the lecture.\n",
    "\n",
    "One note about the problems this week.  In some of the examples there are parameters that influence the size of the computational task involved.  For example a large number of timesteps (or, equivalently, a small timestep on a fixed time interval) will mean that you have to do a lot of computational work.  Changing the number of nodes in a neural network or increasing the size of the data set will each add to the computational effort.  Remember that you can usually halt an execution using the \"Kernel\" menu in Jupyter Notebooks.  As you are performing the exercises always try to run smaller models first and build up to more complicated cases so that you can estimate the execution time.  Juggling the computational load with the accuracy obtained is a real life challenge that we confront whenever we are performing scientific computing tasks so it is not a bad thing to start to get used to it.\n",
    "\n",
    "## Simulating logic gates\n",
    "Binary logical operations like AND, OR, NAND, XOR... are the bread and butter of modern digital devices.  Although it is not of direct practical relevance to simulate their behavior using a neural network, it makes an interesting family of simple problems for learning about neural computation.   \n",
    "\n",
    "As described in lecture, we can treat a logic gate as a two-input function f(x1,x2) where x1 and x2 are regarded as real numbers approximating binary states (0 or 1) and the output is also an approximation of a binary state.   Here we will use a two-layer network (one hidden layer with two nodes) and the sigmoidal function for both activation and output gates.   To use this within the setting of an Euler method we need to be able to compute a loss function L(theta), where theta is a vector of parameters of the network, and its negative gradient F=-nabla L.   These calculations are performed by the following code.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math as math\n",
    "import matplotlib.pyplot as plt\n",
    "font = {'family' : 'helvetica','size'   : 16}\n",
    "plt.rc('font', **font)  # gives bigger fonts\n",
    "\n",
    "def sig_act(s):\n",
    "    return 1.0/(1.0+math.exp(-s))\n",
    "\n",
    "def sig_act_prime(s):\n",
    "    return math.exp(-s)/(1.0+math.exp(-s))**2\n",
    "\n",
    "def p_force(theta):\n",
    "\n",
    "# Calculate negative gradient of L2 loss function (and the loss) for \n",
    "# a two input function modelled with a two layer (2 hidden node) network\n",
    "# using sigmoidal activation\n",
    "# input - parameter vector *theta*\n",
    "# output - negative gradient of L2 loss *F* and *loss*\n",
    "\n",
    "    #change to weight, bias notation\n",
    "    w11 = theta[0]; w12 = theta[1]; w21 = theta[2]; w22 =theta[3]\n",
    "    w31 = theta[4]; w32 = theta[5]; b1 = theta[6]; b2=theta[7]; b3=theta[8]\n",
    "    g = np.zeros((9,1));           # g = gradient - start with a zero vector \n",
    "    loss = 0                       # training loss\n",
    "\n",
    "    for i in range(len(tdata)):\n",
    "        x1 = tdata[i][0]; x2 = tdata[i][1]; c = tdata[i][2];\n",
    "   \n",
    "        # calculate intermediates and output of network\n",
    "        u1 = w11*x1 + w12*x2 + b1\n",
    "        u2 = w21*x1 + w22*x2 + b2\n",
    "      \n",
    "        z1 = sig_act(u1)            # hidden node outputs\n",
    "        z2 = sig_act(u2)\n",
    "        u3 = w31*z1 + w32*z2 +b3    # 2nd layer\n",
    "        \n",
    "        out = sig_act(u3)\n",
    "        res = c-out\n",
    "        loss = loss + res**2        # update the L2 loss\n",
    "        \n",
    "        # now calculate the loss gradient components in reverse order\n",
    "        h3 = -2*res*sig_act_prime(u3) \n",
    "        \n",
    "        g[8] += h3                  # deriv with respect to bias b3\n",
    "        g[5] += h3*z2               # wrt w31\n",
    "        g[4] += h3*z1               # wrt w32\n",
    "        \n",
    "        h2 = h3* sig_act_prime(u2)  # multipliers\n",
    "        h1 = h3* sig_act_prime(u1)\n",
    "        \n",
    "        g[7] += h2*w32              # wrt b2\n",
    "        g[6] += h1*w31              # wrt b1\n",
    "        g[3] += h2*w32*x2           # wrt w22\n",
    "        g[2] += h2*w32*x1           # wrt w21\n",
    "        g[1] += h1*w31*x2           # wrt w12\n",
    "        g[0] += h1*w31*x1           # wrt w11\n",
    "\n",
    "# negate the gradient to get the descent direction\n",
    "\n",
    "    F = -g\n",
    "\n",
    "    return F,loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code can be used to model any two-input function with an output between 0 and 1.\n",
    "\n",
    "Note the following:\n",
    "<ul>\n",
    "    <li> that the data is provided in a global array tdata which has k rows and in each row 3 elements: x1, x2, c, where c is the prescribed value of the function at the point (x1,x2);</li>\n",
    "    <li> that the code begins by translating the vector theta of all parameters into the various parameter names (wij, the weights, and b_i, the biases) according to the diagram you can find for the XOR gate in the slides of lectures 5 and 6.</li>\n",
    "    <li> that the code automatically determines the length of the tdata array and this can be arbitrarily long; </li>\n",
    "    <li> the computation is handled in two steps for each data point.  First the network nodes are sequentially calculated (from left to right), then the components of the gradient are calculated for right to left, i.e. in reverse order.</li>\n",
    "    <li> that both the negative gradient (summed over the data) and the loss are computed by this function.</li>\n",
    "    </ul>\n",
    "\n",
    "### Exercise 1 \n",
    "Test out the code as follows.  Define an array tdata of the following form:\n",
    "$$ \\textrm{tdata} =\\left [ \\begin{array}{ccc} 0 & 0  & 0\\\\0 & 1 & 1\\\\1 & 0 & 1\\\\1& 1 & 1\\end{array}\\right ].$$  (Notice that this is the logic table of a logical OR gate.)  Set the parameter values as follows: (a) $\\theta=0$ (the zero vector in 9 dimensions).  (b) $\\theta =[0,0,0,0,0,2\\log_e(3),100,0,0]$.  What is the value you obtain for the loss and gradient vector in each of cases (a) and (b)?  (c) Does the loss value in each case agree with what you expect? (d) In example (a) give a vector in parameter space in the direction of which you would expect the loss to decrease and briefly (one sentence) explain why this is the case. (2 marks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdata = # !..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Euler's Method\n",
    "Euler's method can be used to numerically integrate the gradient system for the neural network.  The following codes are similar to methods that have been given earlier in the course. They are simply modified to pass and store the loss values as well as the vector field.  Study the codes and make sure that you understand what they are doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Euler(q_n, h, vector_field):\n",
    "# computes a step using Euler's method for the given vector field\n",
    "    F,loss = vector_field(q_n)\n",
    "    return q_n + h*F, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_trajectory( q0, Nsteps, h,  step_function, vector_field):\n",
    "# computes a trajectory of length *Nsteps*\n",
    "# for a differential equation defined by *vector_field*\n",
    "# starting from a given initial condition *q0*\n",
    "# using a given method defined by *step_function*\n",
    "# with stepsize *h*\n",
    "#\n",
    "# outputs independent variable values *t_traj*, \n",
    "# trajectory *q_traj* whose jth column is the solution at time level j\n",
    "\n",
    "    # initialize trajectory\n",
    "    q_traj = [q0]; t_traj = [0]\n",
    "    F,loss = vector_field(q0)\n",
    "    loss_traj = [loss]\n",
    "\n",
    "    # starting values\n",
    "    q = q0; t = 0\n",
    "    \n",
    "    # loop\n",
    "    for n in range(Nsteps):\n",
    "        # calculate next step using provided stepping function\n",
    "        q,loss  = step_function(q, h,  vector_field)\n",
    "        t +=  h\n",
    "\n",
    "        # update trajectory\n",
    "        q_traj.append(q)\n",
    "        t_traj.append(t)\n",
    "        loss_traj.append(loss)\n",
    "\n",
    "    return q_traj, t_traj, loss_traj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the code will accept as inputs a vector field (vector_field) and a method (step_function), as well as a stepsize, initial state, and number of timesteps.  The code outputs q_traj, an array whose columns are the steps in parameter space, t_traj, a corresponding sequence of time levels and loss_traj which is the training loss computed at each of the points.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next specify the OR gate by prescribing the tdata array as mentioned above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdata = np.asarray([[0,0,0],[0,1,1],[1,0,1],[1,1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 \n",
    "Let us attempt to train the network to solve the OR gate.    (a) Compute a trajectory for the neural network by calling the compute_trajectory code.  You should start with an initial random vector which you can obtain by q0 = np.random.random((9,1)).  Use a stepsize of 0.1 and take 100 steps.   (b) plot the training loss as a function of time along this trajectory.    (c) change the stepsize to  1 and repeat steps (a) and (b).  (d) For the stepsize of 1, change the number of steps to 1000 and repeat steps (a) and (b).  Briefly discuss the graphs--what do you think they tell us about the learning process in this case? (marks 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next part, we will look at the classifier obtained by training the perceptron model for an OR gate. To do this, we will want to graph the function defined by the perceptron (for varying inputs x1, x2) for given parameter values.  This can be achieved by making a surface plot or by using pcolor. \n",
    "\n",
    "The first step is to program the raw classifier as an input-output relation.  The code is like the first part of the gradient calculation but here the x1,x2 values are inputs not prescribed data points.  The theta vector is assumed to be provided.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_classifier(x1,x2,theta):\n",
    "# compute 2-layer planar perceptron classifier for given inputs x1, x2\n",
    "# parameter theta: a nine component vector\n",
    "\n",
    "# change to weight, bias notation\n",
    "    w11 = theta[0]; w12 = theta[1]; w21 = theta[2]; w22 =theta[3]\n",
    "    w31 = theta[4]; w32 = theta[5]; b1 = theta[6]; b2=theta[7]; b3=theta[8]\n",
    "\n",
    "    u1 = w11*x1 + w12*x2 + b1\n",
    "    u2 = w21*x1 + w22*x2 + b2\n",
    "      \n",
    "    z1 = sig_act(u1)            # hidden node outputs\n",
    "    z2 = sig_act(u2)\n",
    "    u3 = w31*z1 + w32*z2 +b3    # 2nd layer\n",
    "        \n",
    "    \n",
    "    out = sig_act(u3)         # output\n",
    "\n",
    "    return(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the classifier coded, it is a simple matter of generating a grid of points and evaluating the classifier at each gridpoint.  Functions to do these tasks are given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_points(rectangle,delx):\n",
    "    a = rectangle[0]; b=rectangle[1]; c=rectangle[2]; d=rectangle[3]\n",
    "    x1 = np.arange(a, b+delx, delx)\n",
    "    x2 = np.arange(c, d+delx, delx)\n",
    "    x1grid, x2grid = np.meshgrid(x1, x2)\n",
    "    return x1grid,x2grid\n",
    "\n",
    "def compute_classifier(x1grid,x2grid,theta):\n",
    "    # draw a 3-d plot of the classifier generated by\n",
    "    # the 2-layer pereceptron network\n",
    "    # domain defined by *rectangle* [a,b,c,d] a<=x1<=b; c<=x2<=d\n",
    "    # *delx*: increment in each variable (fineness of grid used for plotting)\n",
    "    # *theta*: parameter vector\n",
    "    dims = x1grid.shape\n",
    "    n1 = dims[0]; n2 = dims[1]\n",
    "    zgrid = np.zeros((n1,n2))\n",
    "    for i in range(n1):\n",
    "        for j in range(n2):\n",
    "            zgrid[i,j] = p_classifier(x1grid[i,j],x2grid[i,j],theta)\n",
    "    return zgrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "Using the codes above, create  pcolor plots (or surface plots) showing the classifiers at the beginning and end of the trajectory computed in Exercise 2e (h=1, N=1000 steps).  You should use the scatter plot capability to visualise the 4 data points that define the OR gate in relation to the pcolor or surface plot (and maybe color them according to opposite of their binary value, 0 or 1, to show a contrast with the classifier). Discuss the graphs obtained and what it tells you about the trained network. (2 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "*This exercise is open ended and individual.*\n",
    " \n",
    "You are now ready to turn your attention to the XOR gate using the same perceptron model. \n",
    "\n",
    "Using the ideas and methods presented above discuss the training of a perceptron model of the XOR gate using gradient descent.   Explore issues such as convergence of the learning process and accuracy of the learned classifier.  Include at least a paragraph of discussion. There is no prescibed model for the solution to this question but you should be careful to justify any statements you make using numerical tests.\n",
    "\n",
    "Please limit the number of graphs provided with your answer to at most 4 and any written text to at most about 200 words. (4 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPD Classifier\n",
    "In the remainder of this assessment we will turn our attention to the SPD matrix classifier which determines, for a matrix of a given class, whether or not the matrix is symmetric positive definite.   Recall that the eigenvalues of a symmetric matrix are all real numbers.   The criterion for a matrix to be SPD is that the spectrum of the matrix (set of all eigenvalues) lies strictly in the right half line ($\\mathbb{R}_+ = \\{x|x\\geq 0\\}$).  Equivalently, we can check if a given symmetric matrix $A$ is SPD by checking if $\\lambda_{\\rm min}(A)>0$.\n",
    "\n",
    "We will set up the SPD classifier as described in Lecture 6.  We restrict ourselves for this purpose to matrices of the form\n",
    "$$ A = \\left [ \\begin{array}{ccc} 1 & a & b\\\\a & 1 & c\\\\b & c & 1\\end{array}\\right ]$$\n",
    "\n",
    "Although there is nothing that would restrict us from applying the same methodology to much larger matrices, or to matrices having a particular sparsity structure, the simplification will keep the computational task more or less manageable without having to use special software frameworks for efficiency.  However, to note, it is still easily possible to overwhelm the computational capabilities of your laptop computer when running the codes described here.   \n",
    "\n",
    "To begin we need to write routines for the activation function and the output function.  We will use ReLU activation and sigmoidal output, although these choices are not essential and other choices could easily be explored.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_a(s):\n",
    "    return s*(s >= 0)\n",
    "\n",
    "def phi_a_prime(s):\n",
    "    return 1*(s >= 0)\n",
    "\n",
    "def phi_o(s):\n",
    "    return 1.0/(1.0+math.exp(-s))\n",
    "\n",
    "def phi_o_prime(s):\n",
    "    return math.exp(-s)/(1.0+math.exp(-s))**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the way the ReLU is programmed is a little unusual--for input s it is given as a product of s and a conditional expression which we normally think of as evaluating to true or false!  In python we are using a trick here that a logical expression can be 'recast' as an integer value (0 or 1) by simply combining it with a number through an arithmetic operation such as multiplication.  We have also used the fact that the 'product' of two python vectors of the same size is another vector of the same size whose elements are formed by taking the products of the corresponding elements.   An advantage of writing the formula in this way is that it is automatically 'vectorised', meaning that I can apply it to a vector and get a vector of results back.  If i used a more standard \"if then else\" approach it would not be possible to do this!  \n",
    "\n",
    "### Exercise 5\n",
    "We are now ready to code the force (negative gradient) in the case of the SPD classifier problem.  A template code is given below which you need to edit to implement the formulas given in Lecture 6.    The parts indicated below in the SPD_force routine that are missing compute the following partial derivatives:  g_z (partial gradient of loss with respect to the vector of hidden nodes), g_u (partial gradient with respect to the first intermediate u=Wx+b), g_W (partial gradient with respect to matrix argument W-- the weights in the first layer), g_b (partial gradient with respect to biases in the first layer).  Study Slide 18  of Lecture 6 and fill in these missing pieces. (2 marks)\n",
    "\n",
    "In case it is unclear, the notation g_W refers to the matrix whose elements are the partial derivatives with respect to each of the components of W, arranged in the same shape as W.\n",
    "\n",
    "Note that the number of hidden layer nodes m need not be passed in explicitly.  It can be figured out from the dimension of theta as long as theta is of the correct form (5m+1 dimensional).  This just simplifies the calling sequence, but we need to check that the dimension indeed is of this form, as shown in the code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SPD_force(theta):\n",
    "    d = len(theta)\n",
    "    # we assume d = 3xm +m + m + 1 = 5m+1\n",
    "    # check this!\n",
    "    if (d-1) %5 != 0:\n",
    "        print('failure - wrong dimension - theta must be a 5m+1 dimensional vector')\n",
    "        raise\n",
    "    \n",
    "    m = (d-1)//5                       # number of internal (hidden layer) nodes\n",
    "    W = theta[0:3*m]\n",
    "    W = np.reshape(W,((m,3)))          # reshape the weight matrix for the hidden layer\n",
    "    wo = theta[3*m:4*m]                # separate the weights of the output layer\n",
    "    b = theta[4*m:5*m]                 # biases of the hidden layer\n",
    "    bo = theta[5*m]                    # bias of the output layer\n",
    "    \n",
    "    loss = 0                           # loss\n",
    "    g = np.zeros((5*m+1,1))            # gradient\n",
    "    \n",
    "    # sum over training data \n",
    "    for i in range(len(tdata)):\n",
    "        xi = np.reshape(tdata[i, 0:3],((3,1)))\n",
    "        ci = tdata[i,3]\n",
    "        u = W @ xi + b\n",
    "        z = phi_a(u)\n",
    "        uo = np.transpose(wo) @ z + bo\n",
    "        c = phi_o(uo)\n",
    "        \n",
    "        res = c-ci                     # swapped from xor example where we had (ci-c)^2\n",
    "        loss += res**2\n",
    "        \n",
    "        g_o = 2*res*phi_o_prime(uo)    # the gradient implemented just as in slides of L6\n",
    "        g_bo = g_o\n",
    "        g_uo = g_o\n",
    "        g_wo = g_o*z\n",
    "        \n",
    "        g_z = # ! Your code here\n",
    "        g_u = # ! Your code here\n",
    "        g_W = # ! Your code here\n",
    "        g_b = # ! Your code here\n",
    "        \n",
    "        gg = np.vstack((np.reshape(g_W,((3*m,1)))))\n",
    "        gg = np.vstack((gg,g_wo))\n",
    "        gg = np.vstack((gg,g_b))\n",
    "        gg = np.vstack((gg,g_bo))\n",
    "        g += gg\n",
    "        \n",
    "    return -g,loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a good idea to also code up the classifier as a function which can be applied to an input x =[[a],[b],[c]] describing the matrix, where it is assumed that all the parameters are given.  We provide this in the function SPD_classifier, below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SPD_classifier(a,b,c,theta):\n",
    "# compute 2-layer SPD classifier for given input matrix with coefficients defined by (a,b,c)\n",
    "# parameter theta: a 5*m+1 component parameter vector\n",
    "    d = len(theta)\n",
    "    # we assume d = 3xm +m + m + 1 = 5m+1\n",
    "    # check this!\n",
    "    if (d-1) %5 != 0:\n",
    "        print('failure - wrong dimension - theta must be a 5m+1 dimensional vector')\n",
    "        raise\n",
    "    \n",
    "    x = np.asarray([[a],[b],[c]])\n",
    "    m = (d-1)//5                       # number of internal (hidden layer) nodes\n",
    "    W = theta[0:3*m]\n",
    "    W = np.reshape(W,((m,3)))          # reshape the weight matrix for the hidden layer\n",
    "    wo = theta[3*m:4*m]                # separate the weights of the output layer\n",
    "    b = theta[4*m:5*m]                 # biases of the hidden layer\n",
    "    bo = theta[5*m]                    # bias of the output layer\n",
    "    u = W @ x + b\n",
    "    z = phi_a(u)\n",
    "    uo = np.transpose(wo) @ z + bo\n",
    "    c = phi_o(uo)\n",
    "    return(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6 \n",
    "It is time to generate our data set.  You will write a function for this purpose.   \n",
    "\n",
    "The matrices are described by three coordinates (a,b,c) and an output class label l (l=1 if SPD, l=0 otherwise).    We wish to generate the matrices randomly, so we need to compute random values of (a,b,c) that are uniformly distributed in the interval [-1,1]. The function np.random.random((n1, n2)) creates a matrix of dimensions n1 rows and n2 columns, each of whose elements are uniformly distributed random numbers in the interval [0,1].   The function np.linalg.eigvals(A) computes the vector of all the eigenvalues of a square matrix A.  Using these two routines, right code to construct an array tdata of given length of the form\n",
    "$$ \\textrm{tdata} = \\left [ \\begin{array}{cccc} a_1 & b_1 & c_1 & l_1\\\\\n",
    "a_2 & b_2 & c_2 & l_2\\\\\n",
    "a_3 & b_3 & c_3 & l_3\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "a_k & b_k & c_k & l_k\n",
    "\\end{array}\n",
    "\\right ]\n",
    "$$\n",
    "$l_j$ is the class label which is just 0 if the corresponding matrix (with 1s on the diagonal) is not SPD and 1 if it is SPD. (2 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use your generate_data code to create a training data set by the line\n",
    "\n",
    "    tdata = generate_data(50)\n",
    "\n",
    "50 is the number of rows in tdata and thus the size of the data set.  This might not be enough! Remember that each one of these data points will make a contribution to the loss function and to the gradient, so if we have too many elements in the data set the computational costs will become large.  As a rule you should probably work with data sets up to at most a few hundred training points in most of your calculations. (Of course if you wish to explore a larger one you could let the program run overnight on your computer!!)\n",
    "\n",
    "Finally, once you have created tdata, you will be able to attempt to train our classifier using compute_trajectory, the Euler method for the step_function, and the SPD_force function that we produced above.   \n",
    "\n",
    "Note that the only place the number of hidden nodes of the network is specified is in the dimension of the parameter vector. This dimension should be 5m+1, in which case m is the number of hidden nodes.  Below we use 51 for this dimension so there are 10 nodes in the hidden layer.  Again the choice of a larger number of nodes (by specifying a larger parameter vector) will lead to greater computational costs.\n",
    "\n",
    "In the graph shown below, we plot the training loss against the time.  It is important to note that the actual training loss curve will be very different depending on the training data and the initial condition, both of which are random!  This means you might want to re-run the method several times to get a better understanding of how it is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdata = generate_data(50)\n",
    "theta,t,loss = compute_trajectory(np.random.random((51,1)),1000,.1,Euler,SPD_force)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(t,loss)\n",
    "plt.xlabel('time'); plt.ylabel('training loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7 \n",
    "Notice that--surprisingly--the training loss increases as well as decreases along the trajectory!  Why might this happen?  Recall that the gradient descent flow (the differential equation) has the property that the loss is a monotone decreasing function.  Give an explanation here for the loss of monotonicity.  (1 mark)\n",
    "\n",
    "The fact that the training loss decreases (eventually), means that we gain high accuracy on the training set.  There is, however, the important question of how the classifier performs on unseen 'test' data.  \n",
    "\n",
    "To examine this, generate a new sample of test data (another tdata array with, say, 100 rows) and calculate the value of the classifier for each element.  Compare this with the actual label (determined using the eigenvalue test).   You may wish to plot the classification errors using plt.bar (which produces a bar graph).  Discuss the result. (1 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8\n",
    "This part is free form and individual.\n",
    "\n",
    "Explore the SPD classification network by using numerical experiments.   You should adjust the network size, the data set size, the number of iterations and the stepsize in order to understand how these alter the convergence or change the results of classification (and the error on test data).  What is the best result you can achieve?   (Explain what \"best\" means!)\n",
    "\n",
    "You may wish to consider any of the following issues and aspects in your discussion.\n",
    "\n",
    "<ul>\n",
    "    <li> the accuracy on the training data, in relation to the number of nodes in the hidden layer, or number of training data points, or training iterations </li>\n",
    "    <li> the accuracy of the trained network on test data, in relation to the number of nodes in the hidden layer, or number of training data points, or training iterations </li>\n",
    "    <li> the use of subsampling from the training data, as mentioned in Lecture 6, in which the SPD_force is modified so that a randomly chosen subset of the training data is used to compute the loss at each step (rescaled by the ratio of the size of the training set to the size of the subsampled training set). </li>\n",
    "    <li> the choice of initial parameters used and its effect on the results obtained, or dependence on the data set used </li>\n",
    "    </ul>\n",
    "\n",
    "(Any *one* of these would be interesting!)\n",
    "\n",
    "Please limit the number of graphs provided with your answer to at most 4 and any written text to at most about 200 words. (4 marks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
